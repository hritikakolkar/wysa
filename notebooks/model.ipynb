{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hritik/Documents/mlops/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import wandb\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhritikakolkar\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, tweet, emotion, tokenizer, max_length):\n",
    "        self.tweet = tweet\n",
    "        self.emotion = emotion\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.tweet[idx],\n",
    "            max_length = self.max_length,\n",
    "            add_special_tokens=True,\n",
    "            padding = \"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            )\n",
    "\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'emotion_labels': torch.tensor(self.emotion[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, config = None, train=True):\n",
    "    if train:\n",
    "        df = df.rename(columns={\n",
    "            'tweet_text': 'tweet',\n",
    "            'emotion_in_tweet_is_directed_at': 'entity', # entity means brand, product or service\n",
    "            'is_there_an_emotion_directed_at_a_brand_or_product': 'emotion'\n",
    "        })\n",
    "        df = df[df[\"tweet\"].notna()].reset_index(drop = True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.replace(pat = r'[^a-zA-Z0-9#@\\!$%^&*(){}:\\-\\'\\\":;,\\.?/\\s]', repl = ' ', regex=True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.replace(pat = r'[:;,\\.?/\\s]{2,}', repl = ' ', regex= True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.strip()\n",
    "        df = df.drop(\"entity\", axis = 1)\n",
    "        df[\"emotion\"] = df[\"emotion\"].replace(config[\"emotion_to_id\"])\n",
    "    else:\n",
    "        df = df.rename(columns={\n",
    "            'Tweet': 'tweet'\n",
    "        })\n",
    "        df = df[df[\"tweet\"].notna()].reset_index(drop = True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.replace(pat = r'[^a-zA-Z0-9#@\\!$%^&*(){}:\\-\\'\\\":;,\\.?/\\s]', repl = ' ', regex=True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.replace(pat = r'[:;,\\.?/\\s]{2,}', repl = ' ', regex= True)\n",
    "        df[\"tweet\"] = df[\"tweet\"].str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_classes_emotion, pretrained_model_path):\n",
    "        super(EmotionClassifier, self).__init__()\n",
    "        self.model_config = RobertaConfig.from_pretrained(pretrained_model_path)\n",
    "        self.pretrained_model = RobertaModel.from_pretrained(pretrained_model_path, config= self.model_config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # We can use single classifier with classes = num_classes_entity + num_classes_emotion, but using this approach for simplification\n",
    "        self.classifier_emotion = nn.Linear(self.pretrained_model.config.hidden_size, num_classes_emotion)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooler_output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        pooler_output= nn.ReLU()(pooler_output)\n",
    "        pooler_output= self.dropout(pooler_output)\n",
    "\n",
    "        # Output for emotion classification\n",
    "        emotion_output = self.classifier_emotion(pooler_output)\n",
    "        # No need of output_probs as using nn.CrossEntropyLoss\n",
    "        # emotion_output_probs = nn.Softmax(dim=1)(emotion_output)\n",
    "\n",
    "        return emotion_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, targets, config):\n",
    "    # Convert tensors to numpy arrays\n",
    "    if isinstance(predictions, np.ndarray) and isinstance(targets, np.ndarray):\n",
    "        predictions_np = predictions\n",
    "        targets_np = targets\n",
    "    else:\n",
    "        predictions_np = predictions.cpu().numpy()\n",
    "        targets_np = targets.cpu().numpy()\n",
    "\n",
    "    labels = np.unique(predictions_np)\n",
    "    # Compute metrics\n",
    "    precision = precision_score(targets_np, predictions_np, average= config[\"average\"], labels= labels, zero_division= config[\"zero_division\"])\n",
    "    recall = recall_score(targets_np, predictions_np, average= config[\"average\"], labels= labels, zero_division= config[\"zero_division\"])\n",
    "    accuracy = accuracy_score(targets_np, predictions_np)\n",
    "    f1 = f1_score(targets_np, predictions_np, average= config[\"average\"], labels= labels, zero_division= config[\"zero_division\"])\n",
    "\n",
    "    return precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_config(config):\n",
    "    train = preprocessing(pd.read_excel(\"../data/dataset.xlsx\", sheet_name = \"Train\"), config= config, train=True)\n",
    "    # test = preprocessing(pd.read_excel(\"data/dataset.xlsx\", sheet_name = \"Test\"), train=False)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train[\"tweet\"].to_numpy(), train[\"emotion\"].to_numpy(), test_size=config[\"test_size\"], random_state= config[\"seed\"])\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(config[\"pretrained_model_path\"])\n",
    "    tokenizer.save_pretrained(config[\"model_save_path\"])\n",
    "    tokenizer.save_pretrained(config[\"model_save_path\"])\n",
    "    \n",
    "    train_dataset = EmotionDataset(tweet= x_train, emotion= y_train, tokenizer= tokenizer, max_length= config[\"max_length\"])\n",
    "    test_dataset = EmotionDataset(tweet= x_test, emotion= y_test, tokenizer= tokenizer, max_length= config[\"max_length\"])\n",
    "\n",
    "    train_dataloader = DataLoader(dataset= train_dataset, batch_size= 8)\n",
    "    test_dataloader = DataLoader(dataset= test_dataset, batch_size= 8)\n",
    "\n",
    "    model = EmotionClassifier(config[\"num_classes_emotion\"], config[\"pretrained_model_path\"])\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    return model, train_dataloader, test_dataloader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, train_dataloader, test_dataloader, criterion, optimizer, config):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    model.to(config[\"device\"])\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    samples_count = 0\n",
    "    global_step = 0\n",
    "    best_f1_score = 0.0\n",
    "    for epoch in tqdm(range(config[\"num_epochs\"])):\n",
    "        model.train()\n",
    "        log_batch_loss = 0\n",
    "        log_batch_emotion_output = np.array([])\n",
    "        log_batch_emotion_labels = np.array([])\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(config[\"device\"])\n",
    "            attention_mask = batch['attention_mask'].to(config[\"device\"])\n",
    "            emotion_labels = batch['emotion_labels'].to(config[\"device\"])\n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            emotion_output = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(emotion_output, emotion_labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            global_step += 1\n",
    "            samples_count += len(emotion_output)\n",
    "\n",
    "            log_batch_loss += float(loss)\n",
    "            log_batch_emotion_output = np.hstack((log_batch_emotion_output, torch.argmax(emotion_output, dim=1).cpu().numpy()))\n",
    "            log_batch_emotion_labels = np.hstack((log_batch_emotion_labels, emotion_labels.cpu().numpy()))\n",
    "            if (global_step + 1) % config[\"log_batch_step\"] == 0:\n",
    "                precision, recall, accuracy, f1 = compute_metrics(log_batch_emotion_output, log_batch_emotion_labels, config)\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\" : log_batch_loss/config[\"log_batch_step\"],\n",
    "                    \"global_step\" : global_step,\n",
    "                    \"samples_count\" : samples_count, \n",
    "                    \"train_precision\": precision,\n",
    "                    \"train_recall\": recall,\n",
    "                    \"train_f1_score\": f1,\n",
    "                    \"train_accuracy\": accuracy\n",
    "                }, step = global_step)\n",
    "                log_batch_loss = 0\n",
    "                log_batch_emotion_output = np.array([])\n",
    "                log_batch_emotion_labels = np.array([])\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            loss = 0\n",
    "            test_metrics = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "            for batch in test_dataloader:\n",
    "                input_ids = batch['input_ids'].to(config[\"device\"])\n",
    "                attention_mask = batch['attention_mask'].to(config[\"device\"])\n",
    "                emotion_labels = batch['emotion_labels'].to(config[\"device\"])\n",
    "\n",
    "                emotion_output = model(input_ids, attention_mask)\n",
    "\n",
    "                batch_metrics = np.array(compute_metrics(torch.argmax(emotion_output, dim=1), emotion_labels, config))\n",
    "                test_metrics += batch_metrics\n",
    "                loss += criterion(emotion_output, emotion_labels)\n",
    "            test_precision, test_recall, test_accuracy, test_f1 = test_metrics/ len(test_dataloader)\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"test_loss\" : float(loss)/len(test_dataloader),\n",
    "                \"test_precision\": test_precision,\n",
    "                \"test_recall\": test_recall,\n",
    "                \"test_f1_score\": test_f1,\n",
    "                \"test_accuracy\": test_accuracy\n",
    "            }, step = global_step)\n",
    "\n",
    "        if epoch == 0 or (epoch+1)%config[\"model_save_epoch\"] == 0 or epoch == config[\"num_epochs\"]-1:\n",
    "            torch.save(model.state_dict(), os.path.join(config[\"model_save_path\"],f\"iter_epoch_{str(epoch).zfill(3)}.bin\"))\n",
    "        \n",
    "        if test_f1 >= best_f1_score:\n",
    "            best_epoch = epoch\n",
    "            best_f1_score = test_f1\n",
    "            torch.save(model.state_dict(), os.path.join(config[\"model_save_path\"],\"pytorch_model.bin\"))\n",
    "    print(f\"Best Epoch for best f1_score of {best_f1_score} is {best_epoch}. Model saved as pytorch_model.bin in the respective version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters, name):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"wysa\", name= name, config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_dataloader, test_dataloader, criterion, optimizer = make_config(config)\n",
    "        \n",
    "        # and use them to train the model\n",
    "        train_and_eval(model, train_dataloader, test_dataloader, criterion, optimizer, config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"test\"\n",
    "config = dict(\n",
    "    seed = 0,\n",
    "    log_batch_step = 10,\n",
    "    num_epochs=1,\n",
    "    num_classes_emotion=3,\n",
    "    num_classes_entity=0,\n",
    "    pretrained_model_path= \"../weights/twitter-roberta-base-sentiment-latest\",\n",
    "    model_save_path = f\"../weights/{version}\",\n",
    "    model_save_epoch = 2,\n",
    "    model_architecture = \"Roberta\",\n",
    "    model_class= \"EmotionClassifier\",\n",
    "    dataset_class = \"EmotionDataset\",\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    max_length = 70,\n",
    "    test_size = 0.2,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    emotion_to_id = {\n",
    "        'No emotion toward brand or product': 0,\n",
    "        'Positive emotion': 1,\n",
    "        'Negative emotion': 2,\n",
    "        \"I can't tell\": 2\n",
    "    },\n",
    "    entity_to_id = {\n",
    "        'iPad': 0,\n",
    "        'iPad or iPhone App': 1,\n",
    "        'iPhone': 3,\n",
    "        'Apple': 4,\n",
    "        'Other Apple product or service': 5,\n",
    "        'Android': 6,\n",
    "        'Android App': 7,\n",
    "        'Google': 8,\n",
    "        'Other Google product or service': 9,\n",
    "    },\n",
    "    average=\"macro\",\n",
    "    zero_division=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hritik/Documents/mlops/notebooks/wandb/run-20231228_144926-o2ztyeid</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hritikakolkar/wysa/runs/o2ztyeid' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/hritikakolkar/wysa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hritikakolkar/wysa' target=\"_blank\">https://wandb.ai/hritikakolkar/wysa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hritikakolkar/wysa/runs/o2ztyeid' target=\"_blank\">https://wandb.ai/hritikakolkar/wysa/runs/o2ztyeid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [02:49<00:00, 169.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch for best f1_score of 0.7383168984331773 is 0. Model saved as pytorch_model.bin in the respective version\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▇▇▅▃▇▁▆▇▃▄▇▄▄█▄▃▄▄▃▆▂▃▇▁▅▅▄▃▆▅▆▃▅▁▄▃▇▃▅▃</td></tr><tr><td>samples_count</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>test_f1_score</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>test_precision</td><td>▁</td></tr><tr><td>test_recall</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▅▅▄▇▄█▃▂▅▆▄▅▅▁▄▆▆▆▆▄▇▇▃▇▄▃▆▅▃▄▅▄▄▆▅▆▄▆▅▆</td></tr><tr><td>train_f1_score</td><td>▇█▂▇▄▆▁▂▇▆▂▆▅▂▇▅▅▄▇▄▅▅▇█▂▃▅▃▁▃▃▄▃▆▄▄▂▃▃█</td></tr><tr><td>train_precision</td><td>▂▆▇▇▄█▃▃▁▆▃▆▆▂▁▇▆▅▂▅▆▆▁▇▄▃▅▄▂▄▆▅▄▆▄▄▂▄▇▃</td></tr><tr><td>train_recall</td><td>▁█▂▅▄▄▁▂▁▅▂▄▄▂▁▄▃▃▁▄▄▃▂▆▂▃▄▃▂▃▂▄▃▅▄▃▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>global_step</td><td>859</td></tr><tr><td>loss</td><td>0.59638</td></tr><tr><td>samples_count</td><td>6870</td></tr><tr><td>test_accuracy</td><td>0.76609</td></tr><tr><td>test_f1_score</td><td>0.73832</td></tr><tr><td>test_loss</td><td>0.56324</td></tr><tr><td>test_precision</td><td>0.73104</td></tr><tr><td>test_recall</td><td>0.79631</td></tr><tr><td>train_accuracy</td><td>0.76923</td></tr><tr><td>train_f1_score</td><td>0.68696</td></tr><tr><td>train_precision</td><td>0.70345</td></tr><tr><td>train_recall</td><td>0.70406</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test</strong> at: <a href='https://wandb.ai/hritikakolkar/wysa/runs/o2ztyeid' target=\"_blank\">https://wandb.ai/hritikakolkar/wysa/runs/o2ztyeid</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231228_144926-o2ztyeid/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model_pipeline(config, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
